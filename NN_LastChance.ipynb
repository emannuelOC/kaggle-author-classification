{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.classify import SklearnClassifier\n",
    "np.random.seed(25)\n",
    "train = pd.read_csv(\"train.csv\", encoding='utf=8')\n",
    "test = pd.read_csv(\"test.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping_target = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train = train.replace({'author':mapping_target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id = test['id']\n",
    "target = train['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import itertools \n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text\n",
    "\n",
    "def cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n",
    "    \n",
    "    txt = str(text.encode('utf-8'))\n",
    "    \n",
    "    txt = re.sub(r'[^A-Za-z\\s]',r' ',txt)\n",
    "    \n",
    "     \n",
    "    if lowercase:\n",
    "        txt = \" \".join([w.lower() for w in txt.split()])\n",
    "        \n",
    "    if remove_stops:\n",
    "        txt = \" \".join([w for w in txt.split() if w not in stops])\n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in txt.split()])\n",
    "    \n",
    "    if lemmatization:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['text'] = train['text'].map(lambda x: preprocess(x))\n",
    "test['text'] = test['text'].map(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].map(lambda x: cleanData(x, lowercase=True, remove_stops=False, stemming=False, lemmatization = False))\n",
    "test['text'] = test['text'].map(lambda x: cleanData(x, lowercase=True, remove_stops=False, stemming=False, lemmatization = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(25)\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, merge, LSTM, Lambda, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 256\n",
    "MAX_NB_WORDS = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 19579 texts.\n",
      "Found 8392 texts.\n"
     ]
    }
   ],
   "source": [
    "n_gram_max = 2\n",
    "print('Processing text dataset')\n",
    "texts_1 = []\n",
    "for text in train['text']:\n",
    "    text = text.split()\n",
    "    texts_1.append(' '.join(add_ngram(text, n_gram_max)))\n",
    "    \n",
    "\n",
    "labels = train['author']  # list of label ids\n",
    "\n",
    "print('Found %s texts.' % len(texts_1))\n",
    "test_texts_1 = []\n",
    "for text in test['text']:\n",
    "    text = text.split()\n",
    "    test_texts_1.append(' '.join(add_ngram(text, n_gram_max)))\n",
    "print('Found %s texts.' % len(test_texts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of data tensor:', (19579, 256))\n",
      "('Shape of label tensor:', (19579,))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_count = 2\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(texts_1)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=True, filters='')\n",
    "tokenizer.fit_on_texts(texts_1)\n",
    "\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "# word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#test_labels = np.array(test_labels)\n",
    "del test_sequences_1\n",
    "del sequences_1\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_words = np.max(data_1) + 1 #min(MAX_NB_WORDS, len(word_index)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM, GRU\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words,20,input_length=MAX_SEQUENCE_LENGTH))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Conv1D(64,\n",
    "#                  5,\n",
    "#                  padding='valid',\n",
    "#                  activation='relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=0, verbose=1, mode='auto')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 25s - loss: 1.0719 - acc: 0.4087 - val_loss: 1.0436 - val_acc: 0.4142\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 25s - loss: 0.9647 - acc: 0.5618 - val_loss: 0.9005 - val_acc: 0.6581\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 24s - loss: 0.7789 - acc: 0.7525 - val_loss: 0.7485 - val_acc: 0.7308\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 25s - loss: 0.6171 - acc: 0.8246 - val_loss: 0.6444 - val_acc: 0.7612\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 24s - loss: 0.4964 - acc: 0.8662 - val_loss: 0.5658 - val_acc: 0.8057\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 24s - loss: 0.4045 - acc: 0.8943 - val_loss: 0.5125 - val_acc: 0.8054\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 24s - loss: 0.3313 - acc: 0.9199 - val_loss: 0.4668 - val_acc: 0.8330\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 24s - loss: 0.2739 - acc: 0.9364 - val_loss: 0.4342 - val_acc: 0.8391\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 24s - loss: 0.2265 - acc: 0.9520 - val_loss: 0.4108 - val_acc: 0.8468\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 24s - loss: 0.1885 - acc: 0.9605 - val_loss: 0.3934 - val_acc: 0.8465\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 24s - loss: 0.1570 - acc: 0.9679 - val_loss: 0.3786 - val_acc: 0.8544\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 24s - loss: 0.1314 - acc: 0.9738 - val_loss: 0.3696 - val_acc: 0.8555\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 24s - loss: 0.1101 - acc: 0.9801 - val_loss: 0.3663 - val_acc: 0.8570\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 26s - loss: 0.0927 - acc: 0.9828 - val_loss: 0.3563 - val_acc: 0.8596\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 31s - loss: 0.0775 - acc: 0.9861 - val_loss: 0.3541 - val_acc: 0.8585\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 30s - loss: 0.0658 - acc: 0.9883 - val_loss: 0.3533 - val_acc: 0.8583\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 29s - loss: 0.0556 - acc: 0.9909 - val_loss: 0.3540 - val_acc: 0.8583\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x116561910>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_1, to_categorical(labels), validation_split=0.2, nb_epoch=25, batch_size=16, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(test_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result['id'] = test_id\n",
    "result['EAP'] = [x[0] for x in preds]\n",
    "result['HPL'] = [x[1] for x in preds]\n",
    "result['MWS'] = [x[2] for x in preds]\n",
    "\n",
    "result.to_csv(\"result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
